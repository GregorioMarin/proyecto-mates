{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copia de Puntua headings-enlaces SERP.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GregorioMarin/proyecto-mates/blob/main/Copia_de_Puntua_headings_enlaces_SERP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Scrapea y punt√∫a headings y enlaces:\n",
        "=> Este colab scrapea los art√≠culos de la SERP y puntua sus headings/enlaces por relevancia con la keyword\n",
        "\n",
        "1. Cambia en el c√≥digo la keyword por lo que quieras (ej. keyword = \"tipos de fiebre\")\n",
        "2. Pulsa el play\n",
        "3. Espera a que el script acabe\n",
        "\n",
        "Debajo del c√≥digo aparecer√°n 2 tablas con las puntuaciones\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "Un saludo desde Malloca,\n",
        "\n",
        "Jose Gris\n"
      ],
      "metadata": {
        "id": "Sd6rVkv_Q71m"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b7C0k3kuQVNi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "80185c72-95f3-4006-ae98-cc951cb6ce17"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Procesando urls:\n",
            "https://medlineplus.gov/spanish/fever.html\n",
            "https://medlineplus.gov/spanish/ency/article/003090.htm\n",
            "https://www.mayoclinic.org/es-es/diseases-conditions/fever/symptoms-causes/syc-20352759\n",
            "https://es.wikipedia.org/wiki/Fiebre\n",
            "https://www.cun.es/enfermedades-tratamientos/enfermedades/fiebre\n",
            "https://kidshealth.org/es/parents/fever.html\n",
            "https://www.salud.mapfre.es/enfermedades/reportajes-enfermedades/temperatura-fiebre/\n",
            "https://www.osakidetza.euskadi.eus/enfermedades-comunes-adultos/-/recomendaciones-utiles-fiebre-en-adultos/\n",
            "8 p√°ginas escrapeadas correctamente\n",
            "Instalando dependencias...\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (3.2.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk) (1.15.0)\n",
            "Requirement already satisfied: gensim in /usr/local/lib/python3.7/dist-packages (3.6.0)\n",
            "Requirement already satisfied: six>=1.5.0 in /usr/local/lib/python3.7/dist-packages (from gensim) (1.15.0)\n",
            "Requirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.7/dist-packages (from gensim) (1.4.1)\n",
            "Requirement already satisfied: smart-open>=1.2.1 in /usr/local/lib/python3.7/dist-packages (from gensim) (5.2.1)\n",
            "Requirement already satisfied: numpy>=1.11.3 in /usr/local/lib/python3.7/dist-packages (from gensim) (1.21.5)\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "Collecting es_core_news_sm==2.2.5\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/es_core_news_sm-2.2.5/es_core_news_sm-2.2.5.tar.gz (16.2 MB)\n",
            "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 16.2 MB 4.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: spacy>=2.2.2 in /usr/local/lib/python3.7/dist-packages (from es_core_news_sm==2.2.5) (2.2.4)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->es_core_news_sm==2.2.5) (4.63.0)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->es_core_news_sm==2.2.5) (2.0.6)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->es_core_news_sm==2.2.5) (2.23.0)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->es_core_news_sm==2.2.5) (3.0.6)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->es_core_news_sm==2.2.5) (0.4.1)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->es_core_news_sm==2.2.5) (7.4.0)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->es_core_news_sm==2.2.5) (1.0.0)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->es_core_news_sm==2.2.5) (1.21.5)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->es_core_news_sm==2.2.5) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->es_core_news_sm==2.2.5) (1.0.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->es_core_news_sm==2.2.5) (57.4.0)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->es_core_news_sm==2.2.5) (0.9.0)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->es_core_news_sm==2.2.5) (1.1.3)\n",
            "Requirement already satisfied: importlib-metadata>=0.20 in /usr/local/lib/python3.7/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->es_core_news_sm==2.2.5) (4.11.2)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->es_core_news_sm==2.2.5) (3.10.0.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->es_core_news_sm==2.2.5) (3.7.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->es_core_news_sm==2.2.5) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->es_core_news_sm==2.2.5) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->es_core_news_sm==2.2.5) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->es_core_news_sm==2.2.5) (2021.10.8)\n"
          ]
        }
      ],
      "source": [
        "#Cambia la keyword por la que quieras (ej. keyword = \"mejores cafeteras 2022\")\n",
        "keyword = \"fiebre\"\n",
        "encabezados = ['h1', 'h2', 'h3']\n",
        "\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import re\n",
        "import nltk\n",
        "\n",
        "class pagina():\n",
        "    def __init__(self, url):\n",
        "        self.url = url\n",
        "        self.texto = \"\"\n",
        "        self.enlaces  = []\n",
        "        self.headings = []\n",
        "\n",
        "class serp():\n",
        "     \n",
        "    # init method or constructor\n",
        "    def __init__(self, query):\n",
        "        self.query = query.replace(\" \", \"+\")\n",
        "        self.ok = False\n",
        "        self.incidendias = []\n",
        "        self.paginas = []\n",
        "        self.start()\n",
        "\n",
        "    def start(self):\n",
        "        URL = \"https://www.google.com/search?hl=es&gl=es&q=%s&oq=%s\" % (self.query, self.query)\n",
        "        headers =  {\"user-agent\" : \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/88.0.4324.104 Safari/537.36\"}\n",
        "        enlacesSerp = []\n",
        "        resp = requests.get(URL, headers = headers)\n",
        "        if resp.status_code == 200: \n",
        "            soup = BeautifulSoup(resp.content, \"html.parser\")\n",
        "            links = soup.find_all(\"div\", {\"class\" : \"g\"})\n",
        "            print(\"Procesando urls:\")\n",
        "            for x in links:\n",
        "                link = x.find(\"a\")['href']\n",
        "                #corrijo feature snippet\n",
        "                if \"#:~:text\" in link: \n",
        "                    link = link.split(\"#:~:text\")[0]\n",
        "                if link.startswith(\"/\"):continue\n",
        "                print(link)\n",
        "\n",
        "                if not link in enlacesSerp:\n",
        "                    enlacesSerp.append(link)\n",
        "                    \n",
        "                    try:\n",
        "                      resp = requests.get(link, headers = headers, timeout=10)\n",
        "                         \n",
        "                    except Exception as e:\n",
        "                      self.incidendias.append(f\"timeout error: {str(e)} url: {link}\")\n",
        "\n",
        "                    soup = BeautifulSoup(resp.content, \"html.parser\")\n",
        "\n",
        "                    if resp.status_code == 200:\n",
        "                        pag = pagina(link)\n",
        "                        \n",
        "                        #elimino scrips\n",
        "                        for s in soup.find_all(['script', 'style']):\n",
        "                            s.extract()\n",
        "                        \n",
        "                        #recupero texto\n",
        "                        texto = \"\"\n",
        "                        tag = soup.body\n",
        "                        for string in tag.strings:\n",
        "                            texto = texto + \" \" + string\n",
        "                        pag.texto = texto\n",
        "\n",
        "                        #recupero enlaces\n",
        "                        for en in soup.find_all(\"a\", href=True):\n",
        "                            if \"#\" in en[\"href\"] or \"http\" in en[\"href\"]:continue\n",
        "                            if en.text not in pag.enlaces:\n",
        "                                pag.enlaces.append(en.text)\n",
        "\n",
        "                        #recupero headings\n",
        "                        for heading in soup.find_all(encabezados):\n",
        "                            if heading.text not in pag.headings:\n",
        "                                pag.headings.append([heading.name, heading.text.strip('\\n').strip()])\n",
        "                        \n",
        "                        self.paginas.append(pag)\n",
        "                    else:\n",
        "                        self.incidendias.append(f\"Status code: {resp.status_code} url: {link}\")\n",
        "\n",
        "            if len(self.paginas) < 5:\n",
        "                self.ok = True\n",
        "                self.incidendias.append(\"Menos de 5 resultados escrapeados\")\n",
        "            else:\n",
        "                self.ok = True\n",
        " \n",
        "        else:\n",
        "            self.ok = False\n",
        "\n",
        "        #reporto incidencias scrapeo\n",
        "        print(str(len(self.paginas)) + \" p√°ginas escrapeadas correctamente\")\n",
        "\n",
        "        if len(self.incidendias) > 0:\n",
        "            print(\"Incidencias\")\n",
        "            print(\"-----------\")\n",
        "            for x in self.incidendias:\n",
        "                print(x)\n",
        "\n",
        "#Aviso para ti, navegante programador => me da pereza limpiar el c√≥digo üòâ\n",
        "scrap = serp(keyword)\n",
        "\n",
        "if scrap.ok == False:\n",
        "    print(\"No se ha completado el scrap, se detiene el an√°lisis => Sorry...\")\n",
        "    quit()\n",
        "\n",
        "#Creo el modelo word2Vec => paso de palabras a vectores\n",
        "print(\"Instalando dependencias...\")\n",
        "!pip install nltk\n",
        "!pip install gensim\n",
        "from gensim.models import Word2Vec\n",
        "from unicodedata import normalize\n",
        "import spacy\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "!python -m spacy download es_core_news_sm\n",
        "import es_core_news_sm\n",
        "nlp = es_core_news_sm.load()\n",
        "\n",
        "print(\"Pasando texto scrapeado a vectores y creando modelo word2Vec...\")\n",
        "texto = \"\"\n",
        "for p in scrap.paginas:\n",
        "    texto = texto + \" \" + p.texto\n",
        "texto = texto.strip()\n",
        "\n",
        "def lematizarFrase(frase):\n",
        "    doc = nlp(frase)\n",
        "    lematizado = [token.lemma_ for token in doc]\n",
        "    return \" \".join(lematizado)\n",
        "\n",
        "def quitarAcentos(frase):\n",
        "  frase = re.sub(\n",
        "        r\"([^n\\u0300-\\u036f]|n(?!\\u0303(?![\\u0300-\\u036f])))[\\u0300-\\u036f]+\", r\"\\1\", \n",
        "        normalize( \"NFD\", frase), 0, re.I\n",
        "    )\n",
        "  frase = normalize( 'NFC', frase)\n",
        "  return frase\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "def quitarStopWords(frase):\n",
        "  tokenizado = frase.split()\n",
        "  tokenizado = [w for w in tokenizado if w not in stopwords.words('spanish')]\n",
        "  return \" \".join(tokenizado)\n",
        "\n",
        "def limpiarBasico(frase, lemat = True):\n",
        "    frase = re.sub('[^a-zA-Z√Ä-√ø\\u00f1\\u00d1]', ' ', frase)\n",
        "    frase = re.sub(r'\\s+', ' ', frase)\n",
        "    frase = frase.replace(\"  \", \" \").strip()\n",
        "    if lemat == True: frase = lematizarFrase(frase)\n",
        "    frase = quitarAcentos(frase)\n",
        "    return frase\n",
        "\n",
        "def limpia(texto):\n",
        "    procesando = limpiarBasico(texto)\n",
        "    procesando = procesando.lower()\n",
        "    #procesando = quitarAcentos(procesando)\n",
        "    procesando = procesando.replace(\"(\", \"\").replace(\")\", \"\")\n",
        "        \n",
        "    frases = nltk.sent_tokenize(procesando)\n",
        "    tokenizado = [nltk.word_tokenize(sent) for sent in frases]\n",
        "\n",
        "    # Removing Stop Words\n",
        "    for i in range(len(tokenizado)):\n",
        "        tokenizado[i] = [w for w in tokenizado[i] if w not in stopwords.words('spanish')]\n",
        "    return tokenizado\n",
        "\n",
        "textoLimpio = limpia(texto)\n",
        "from gensim.models import Word2Vec\n",
        "\n",
        "word2vec = Word2Vec(textoLimpio, min_count=2, iter= 2)\n",
        "modelo = word2vec.wv\n",
        "print(\"Modelo creado => Puntuando headings/enlaces...\")\n",
        "\n",
        "#modelo.vocab\n",
        "vocabulario = modelo.index2word\n",
        "\n",
        "def limpiarFrase(frase):\n",
        "    frase = frase.lower()\n",
        "    frase = frase.replace(\"(\", \"\").replace(\")\", \"\")\n",
        "    frase = limpiarBasico(frase)\n",
        "    frase = frase.split()\n",
        "    frase = [x for x in frase if x in vocabulario]   \n",
        "    return \" \".join(frase).strip()\n",
        "\n",
        "headings = []\n",
        "for pagina in scrap.paginas:\n",
        "    for heading in pagina.headings:\n",
        "      he = []\n",
        "      \n",
        "      he.append(heading[1].strip())\n",
        "      sinStop = quitarStopWords(heading[1])\n",
        "      he.append(limpiarFrase(sinStop))\n",
        "      headings.append(he)\n",
        "\n",
        "enlaces = []\n",
        "for pagina in scrap.paginas:\n",
        "    for enlace in pagina.enlaces:\n",
        "      he = []\n",
        "      he.append(enlace.strip())\n",
        "      sinStop = quitarStopWords(enlace)\n",
        "      he.append(limpiarFrase(sinStop))\n",
        "      enlaces.append(he)\n",
        "\n",
        "\n",
        "headings = [hea for hea in headings if hea[1].strip() !=\"\"]\n",
        "enlaces = [hea for hea in enlaces if hea[1].strip() !=\"\"]\n",
        "\n",
        "keys = [x for x in limpiarBasico(keyword).lower().split() if x in vocabulario]\n",
        "\n",
        "#puntuo los enlaces/headings por relevancia con la keyword\n",
        "def puntuar(keyword, lista):\n",
        "  for en in lista:\n",
        "    palabrasRelev = [palabra for palabra in en[1].split()]\n",
        "    res = round(modelo.n_similarity(keys, palabrasRelev),3)\n",
        "    en.append(str(res))\n",
        "  lista = sorted(lista, key=lambda x: x[2], reverse=True)\n",
        "  return lista\n",
        "  \n",
        "\n",
        "headings = puntuar(keyword, headings)\n",
        "enlaces = puntuar(keyword,enlaces)\n",
        "\n",
        "\n",
        "def agrupar(lista0):\n",
        "  lista = lista0.copy()\n",
        "  headingsAgrupados = []\n",
        "  while len(lista)>0:\n",
        "    relacionados = []\n",
        "    primero = lista.pop(0)\n",
        "    palabrasPrimero = [palabra for palabra in primero[1].split()]\n",
        "    \n",
        "    for hea in list(lista):\n",
        "      palabrasRelev = [palabra for palabra in hea[1].split()]\n",
        "      res = round(modelo.n_similarity(palabrasPrimero, palabrasRelev),3)\n",
        "      \n",
        "      if res == 1:\n",
        "        #print(str(res), palabrasPrimero, palabrasRelev)\n",
        "        if hea[0] not in relacionados and hea[0] != primero[0]:\n",
        "          relacionados.append(hea[0])\n",
        "        lista.remove(hea)\n",
        "    relacionados = \" - \".join(relacionados)\n",
        "    primero.append(\"\")\n",
        "    primero.append(relacionados)\n",
        "    headingsAgrupados.append(primero)\n",
        "  return headingsAgrupados\n",
        "\n",
        "#Agrupo los headings similares\n",
        "headingsFinal = agrupar(headings)\n",
        "\n",
        "#borro duplicados enlaces\n",
        "enlaces2 = []\n",
        "for enlace in enlaces:\n",
        "  anchors = [x[0].lower() for x in enlaces2]\n",
        "  if enlace[0].lower() not in anchors:\n",
        "    enlaces2.append(enlace)\n",
        "enlaces = enlaces2\n",
        "\n",
        "#Paso de listas a tablas\n",
        "import pandas as pd \n",
        "\n",
        "headingsFinal = [heading[:5:2] for heading in headingsFinal]\n",
        "enlacesFinal = [enlace[::2] for enlace in enlaces]\n",
        "\n",
        "#Visualizo tablas\n",
        "from google.colab import data_table\n",
        "def pasarATabla(lista,columnas):\n",
        "  lista = pd.DataFrame (lista, columns = columnas )\n",
        "  lista = data_table.DataTable(lista, include_index=True, num_rows_per_page=20)\n",
        "  display(lista)\n",
        "\n",
        "\n",
        "pasarATabla(headingsFinal, ['Encabezado', 'Puntuaci√≥n', 'Agrupados'])\n",
        "pasarATabla(enlacesFinal, ['Enlaces', 'Puntuaci√≥n'])\n",
        "\n",
        "print(\"-----------------------------------------\")\n",
        "print(\"Que la fuerza te acompa√±e joven Skywalker\")\n",
        "print(\"-----------------------------------------\")\n",
        "\n",
        "#Si quieres visualizar el vocabulario => quita el # de la siguiente l√≠nea y dale al play.\n",
        "#pasarATabla(vocabulario, ['Vocabulario'])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "‚Üë Las headings y enlaces puntuados aparecer√°n arriba => Puedes pulsar el bot√≥n de copiar en cada tabla para descargarlas\n",
        "\n",
        "‚Üì Si quieres descargarte los headings de cada art√≠culo => Pulsa al play en el c√≥digo de abajo"
      ],
      "metadata": {
        "id": "xVZkD5A11DHm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Headings de cada art√≠culo\n",
        "\n",
        "ruta = f'{keyword} - headings.txt'\n",
        "file = open(ruta, 'w', encoding=\"utf-8\")\n",
        "\n",
        "file.write('Keyword: ' + keyword+\"\\n\\n\")\n",
        "for p in scrap.paginas:\n",
        "    file.write(p.url+\"\\n\")\n",
        "    for h in p.headings:\n",
        "        Ntab = \"  \" * int(h[0].replace(\"h\", \"\").replace(\"H\", \"\"))\n",
        "        file.write(f\"{Ntab}{h[0]} - {h[1]}\\n\")\n",
        "    file.write(\"\\n\")\n",
        "\n",
        "from google.colab import files\n",
        "files.download(ruta)\n"
      ],
      "metadata": {
        "id": "pB1aCjBXyzIN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "‚Üë La puntuaci√≥n de headings/enlaces se muestra m√°s arriba ‚Üë"
      ],
      "metadata": {
        "id": "HyFjz1gy1iwz"
      }
    }
  ]
}